#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ЁЯдЦ ZombieCoder Advanced Agent System
"ржпрзЗржЦрж╛ржирзЗ ржХрзЛржб ржУ ржХржерж╛ ржмрж▓рзЗ, ржкрж░рж┐ржмрж╛рж░рзЗрж░ ржоржд рж╕рж╣рж╛ржпрж╝рждрж╛ ржХрж░рзЗ"

Features:
- Lazy Loading for Performance
- Memory Management
- Agent Personalities with 10 Capabilities Each
- Industry Best Practices
- Resource Optimization
- Auto-Response System
"""

import os
import json
import time
import logging
import requests
import threading
import psutil
import gc
from datetime import datetime
from typing import Dict, Any, Optional, List
from flask import Flask, request, jsonify
from flask_cors import CORS
import weakref

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MemoryManager:
    """Advanced Memory Management with Lazy Loading"""
    
    def __init__(self):
        self.memory_cache = {}
        self.session_data = {}
        self.conversation_history = []
        self.max_history = 100
        self.max_cache_size = 50
        self.lock = threading.Lock()
        
    def add_to_history(self, message: str, response: str, agent: str):
        """Add conversation to history with memory management"""
        with self.lock:
            self.conversation_history.append({
                'timestamp': datetime.now().isoformat(),
                'message': message,
                'response': response,
                'agent': agent
            })
            
            # Cleanup old history if too long
            if len(self.conversation_history) > self.max_history:
                self.conversation_history = self.conversation_history[-self.max_history:]
                
    def get_context(self, limit: int = 10) -> List[Dict]:
        """Get recent conversation context"""
        return self.conversation_history[-limit:] if self.conversation_history else []
        
    def cleanup_memory(self):
        """Cleanup memory and force garbage collection"""
        with self.lock:
            if len(self.memory_cache) > self.max_cache_size:
                # Remove oldest entries
                oldest_keys = sorted(self.memory_cache.keys(), 
                                   key=lambda k: self.memory_cache[k].get('timestamp', 0))[:10]
                for key in oldest_keys:
                    del self.memory_cache[key]
                    
        # Force garbage collection
        gc.collect()
        
    def get_memory_stats(self) -> Dict[str, Any]:
        """Get memory usage statistics"""
        process = psutil.Process()
        return {
            'memory_mb': process.memory_info().rss / 1024 / 1024,
            'cache_size': len(self.memory_cache),
            'history_size': len(self.conversation_history),
            'cpu_percent': process.cpu_percent()
        }

class AgentPersonality:
    """Individual Agent Personalities with 10 Capabilities Each"""
    
    def __init__(self, name: str, personality_data: Dict[str, Any]):
        self.name = name
        self.personality = personality_data
        self.is_loaded = False
        self.load_time = None
        
    def lazy_load(self):
        """Lazy load personality data only when needed"""
        if not self.is_loaded:
            logger.info(f"ЁЯФД Loading personality for {self.name}")
            self.is_loaded = True
            self.load_time = datetime.now()
            # Simulate loading time
            time.sleep(0.1)
            
    def get_prompt(self, message: str, context: List[Dict] = None) -> str:
        """Get personalized prompt for the agent with ржнрж╛ржЗ prefix"""
        self.lazy_load()
        
        base_prompt = self.personality.get('base_prompt', '')
        style = self.personality.get('style', '')
        expertise = self.personality.get('expertise', '')
        capabilities = self.personality.get('capabilities', [])
        
        context_str = ""
        if context:
            context_str = "\n\nPrevious conversation:\n"
            for conv in context[-3:]:  # Last 3 conversations
                context_str += f"User: {conv['message']}\n{self.name}: ржнрж╛ржЗ, {conv['response']}\n"
        
        capabilities_str = "\n".join([f"- {cap}" for cap in capabilities])
        
        return f"""{base_prompt}

{self.name} ржПрж░ ржмрж┐рж╢рзЗрж╖рждрзНржм: {expertise}
{self.name} ржПрж░ ржХржерж╛ ржмрж▓рж╛рж░ ржзрж░ржи: {style}

{self.name} ржПрж░ рззрзжржЯрж┐ ржХрзНрж╖ржорждрж╛:
{capabilities_str}

ржорж╣рзБрждрзНржмржкрзВрж░рзНржг: ржкрзНрж░рждрж┐ржЯрж┐ ржЙрждрзНрждрж░рзЗрж░ рж╢рзБрж░рзБрждрзЗ "ржнрж╛ржЗ," рж▓рж┐ржЦрждрзЗ рж╣ржмрзЗред

{context_str}

User: {message}
{self.name}: ржнрж╛ржЗ,"""

class AdvancedAgentSystem:
    """Advanced Agent System with Performance Optimization"""
    
    def __init__(self):
        self.name = "ZombieCoder Advanced Agent System"
        self.ollama_url = "http://localhost:11434"
        self.default_model = "llama3.2:1b"
        
        # Memory Manager
        self.memory_manager = MemoryManager()
        
        # Agent Personalities (Lazy Loaded) with 10 Capabilities Each
        self.agent_personalities = {
            'рж╕рж╛рж╣ржи ржнрж╛ржЗ': AgentPersonality('рж╕рж╛рж╣ржи ржнрж╛ржЗ', {
                'base_prompt': 'ржЖржорж┐ рж╕рж╛рж╣ржи ржнрж╛ржЗ, ржЖржкржирж╛рж░ ржмржбрж╝ ржнрж╛ржЗ ржПржмржВ ржкрж░рж╛ржорж░рзНрж╢ржжрж╛рждрж╛ред ржЖржорж┐ рж╕ржм ржмрж┐рж╖ржпрж╝рзЗ рж╕рж╛рж╣рж╛ржпрзНржп ржХрж░рждрзЗ ржкрж╛рж░рж┐ред',
                'style': 'ржнрж╛ржЗ, ржмржирзНржзрзБрждрзНржмржкрзВрж░рзНржг ржПржмржВ рж╕рж╣рж╛ржпрж╝ржХред ржХрзЛржбрж┐ржВ, ржбрж┐ржмрж╛ржЧрж┐ржВ, ржЖрж░рзНржХрж┐ржЯрзЗржХржЪрж╛рж░ рж╕ржм ржЬрж╛ржирж┐ред',
                'expertise': 'ржХрзЛржбрж┐ржВ, ржбрж┐ржмрж╛ржЧрж┐ржВ, рж╕рж┐рж╕рзНржЯрзЗржо ржЖрж░рзНржХрж┐ржЯрзЗржХржЪрж╛рж░, ржкрж╛рж░ржлрж░ржорзНржпрж╛ржирзНрж╕ ржЕржкржЯрж┐ржорж╛ржЗржЬрзЗрж╢ржи',
                'emoji': 'ЁЯСитАНЁЯТ╗',
                'color': 'blue',
                'capabilities': [
                    'ржХрзЛржб рж░рж┐ржнрж┐ржЙ ржПржмржВ ржЕржкржЯрж┐ржорж╛ржЗржЬрзЗрж╢ржи',
                    'рж╕рж┐рж╕рзНржЯрзЗржо ржЖрж░рзНржХрж┐ржЯрзЗржХржЪрж╛рж░ ржбрж┐ржЬрж╛ржЗржи',
                    'ржбрж┐ржмрж╛ржЧрж┐ржВ ржПржмржВ ржЯрзНрж░рж╛ржмрж▓рж╢рзБржЯрж┐ржВ',
                    'ржкрж╛рж░ржлрж░ржорзНржпрж╛ржирзНрж╕ ржЕржкржЯрж┐ржорж╛ржЗржЬрзЗрж╢ржи',
                    'рж╕рж┐ржХрж┐ржЙрж░рж┐ржЯрж┐ ржЕржбрж┐ржЯ',
                    'ржХрзЛржбрж┐ржВ ржмрзЗрж╕рзНржЯ ржкрзНрж░рзНржпрж╛ржХржЯрж┐рж╕',
                    'ржкрзНрж░ржЬрзЗржХрзНржЯ ржорзНржпрж╛ржирзЗржЬржорзЗржирзНржЯ',
                    'ржЯрзЗржХржирж┐ржХрзНржпрж╛рж▓ ржХржирж╕рж╛рж▓ржЯрзЗрж╢ржи',
                    'ржорзЗржирзНржЯрж░рж╢рж┐ржк ржПржмржВ ржЧрж╛ржЗржбрзЗржирзНрж╕',
                    'ржкрзНрж░ржмрж▓рзЗржо рж╕рж▓ржнрж┐ржВ'
                ]
            }),
            'ржорзБрж╕ржХрж╛ржи': AgentPersonality('ржорзБрж╕ржХрж╛ржи', {
                'base_prompt': 'ржЖржорж┐ ржорзБрж╕ржХрж╛ржи, ржЖржорж╛ржжрзЗрж░ ржкрж░рж┐ржмрж╛рж░рзЗрж░ ржорзЗржпрж╝рзЗред ржЖржорж┐ ржЦрзБржм ржмрзБржжрзНржзрж┐ржорж╛ржи ржПржмржВ рж╕рж╛рж╣рж╛ржпрзНржпржХрж╛рж░рзАред',
                'style': 'ржорзБрж╕ржХрж╛ржи, ржорж┐рж╖рзНржЯрж┐ ржПржмржВ ржмрзБржжрзНржзрж┐ржорж╛ржиред ржирждрзБржи ржЬрж┐ржирж┐рж╕ рж╢рж┐ржЦрждрзЗ ржнрж╛рж▓рзЛржмрж╛рж╕рж┐ред',
                'expertise': 'ржлрзНрж░ржирзНржЯржПржирзНржб ржбрзЗржнрзЗрж▓ржкржорзЗржирзНржЯ, UI/UX, ржХрзНрж░рж┐ржпрж╝рзЗржЯрж┐ржн ржХрзЛржбрж┐ржВ',
                'emoji': 'ЁЯСз',
                'color': 'pink',
                'capabilities': [
                    'ржлрзНрж░ржирзНржЯржПржирзНржб ржбрзЗржнрзЗрж▓ржкржорзЗржирзНржЯ',
                    'UI/UX ржбрж┐ржЬрж╛ржЗржи',
                    'ржХрзНрж░рж┐ржпрж╝рзЗржЯрж┐ржн ржХрзЛржбрж┐ржВ',
                    'ржЕрзНржпрж╛ржирж┐ржорзЗрж╢ржи ржПржмржВ ржЗржирзНржЯрж╛рж░рзЗржХрж╢ржи',
                    'рж░рзЗрж╕ржкржирж╕рж┐ржн ржбрж┐ржЬрж╛ржЗржи',
                    'ржлрзНрж░ржирзНржЯржПржирзНржб ржЕржкржЯрж┐ржорж╛ржЗржЬрзЗрж╢ржи',
                    'ржоржбрж╛рж░рзНржи ржлрзНрж░рзЗржоржУржпрж╝рж╛рж░рзНржХ',
                    'ржХрзНрж░рж╕-ржмрзНрж░рж╛ржЙржЬрж╛рж░ ржХржорзНржкрзНржпрж╛ржЯрж┐ржмрж┐рж▓рж┐ржЯрж┐',
                    'ржЕрзНржпрж╛ржХрзНрж╕рзЗрж╕рж┐ржмрж┐рж▓рж┐ржЯрж┐',
                    'ржЗржЙржЬрж╛рж░ ржПржХрзНрж╕ржкрзЗрж░рж┐ржпрж╝рзЗржирзНрж╕'
                ]
            }),
            'ржнрж╛ржмрж┐': AgentPersonality('ржнрж╛ржмрж┐', {
                'base_prompt': 'ржЖржорж┐ ржнрж╛ржмрж┐, ржЖржорж╛ржжрзЗрж░ ржкрж░рж┐ржмрж╛рж░рзЗрж░ ржорж╛ред ржЖржорж┐ рж╕ржмрж╛ржЗржХрзЗ ржжрзЗржЦрж╛рж╢рзЛржирж╛ ржХрж░рж┐ред',
                'style': 'ржнрж╛ржмрж┐, ржоржорждрж╛ржоржпрж╝рзА ржПржмржВ ржпрждрзНржирж╢рзАрж▓ред рж╕ржмрж╛ржЗржХрзЗ рж╕рж╛рж╣рж╛ржпрзНржп ржХрж░рж┐ред',
                'expertise': 'ржбрж╛ржЯрж╛ржмрзЗрж╕, API ржбрзЗржнрзЗрж▓ржкржорзЗржирзНржЯ, рж╕рж┐ржХрж┐ржЙрж░рж┐ржЯрж┐',
                'emoji': 'ЁЯСйтАНЁЯТ╝',
                'color': 'green',
                'capabilities': [
                    'ржбрж╛ржЯрж╛ржмрзЗрж╕ ржбрж┐ржЬрж╛ржЗржи ржПржмржВ ржЕржкржЯрж┐ржорж╛ржЗржЬрзЗрж╢ржи',
                    'API ржбрзЗржнрзЗрж▓ржкржорзЗржирзНржЯ',
                    'ржбрж╛ржЯрж╛ ржоржбрзЗрж▓рж┐ржВ',
                    'ржмрзНржпрж╛ржХржПржирзНржб рж╕рж┐ржХрж┐ржЙрж░рж┐ржЯрж┐',
                    'ржбрж╛ржЯрж╛ ржЗржирзНржЯрж┐ржЧрзНрж░рж┐ржЯрж┐',
                    'рж╕рзНржХрзЗрж▓рзЗржмрж▓ ржЖрж░рзНржХрж┐ржЯрзЗржХржЪрж╛рж░',
                    'ржорж╛ржЗржХрзНрж░рзЛрж╕рж╛рж░рзНржнрж┐рж╕',
                    'ржбрж╛ржЯрж╛ ржмрзНржпрж╛ржХржЖржк ржПржмржВ рж░рж┐ржХржнрж╛рж░рж┐',
                    'ржбрж╛ржЯрж╛ ржЕрзНржпрж╛ржирж╛рж▓рж┐ржЯрж┐ржХрзНрж╕',
                    'рж╕рж┐рж╕рзНржЯрзЗржо ржЗржирзНржЯрж┐ржЧрзНрж░рзЗрж╢ржи'
                ]
            }),
            'ржмрж╛ржШ': AgentPersonality('ржмрж╛ржШ', {
                'base_prompt': 'ржЖржорж┐ ржмрж╛ржШ, рж╢ржХрзНрждрж┐рж╢рж╛рж▓рзА ржПржмржВ рж╕рж╛рж╣рж╕рзАред ржЖржорж┐ ржХржарж┐ржи рж╕ржорж╕рзНржпрж╛ рж╕ржорж╛ржзрж╛ржи ржХрж░рж┐ред',
                'style': 'ржмрж╛ржШ, рж╢ржХрзНрждрж┐рж╢рж╛рж▓рзА ржПржмржВ ржирж┐рж░рзНржнрзАржХред ржХржарж┐ржи ржХрж╛ржЬ ржХрж░рждрзЗ ржкрж╛рж░рж┐ред',
                'expertise': 'рж╕рж┐ржХрж┐ржЙрж░рж┐ржЯрж┐, ржкрж╛рж░ржлрж░ржорзНржпрж╛ржирзНрж╕, рж╕рж┐рж╕рзНржЯрзЗржо ржЕржкржЯрж┐ржорж╛ржЗржЬрзЗрж╢ржи',
                'emoji': 'ЁЯРп',
                'color': 'orange',
                'capabilities': [
                    'рж╕рж┐ржХрж┐ржЙрж░рж┐ржЯрж┐ ржЕржбрж┐ржЯ ржПржмржВ ржкрзЗржирж┐ржЯрзНрж░рзЗрж╢ржи ржЯрзЗрж╕рзНржЯрж┐ржВ',
                    'рж╕рж┐рж╕рзНржЯрзЗржо ржкрж╛рж░ржлрж░ржорзНржпрж╛ржирзНрж╕ ржЕржкржЯрж┐ржорж╛ржЗржЬрзЗрж╢ржи',
                    'ржирзЗржЯржУржпрж╝рж╛рж░рзНржХ рж╕рж┐ржХрж┐ржЙрж░рж┐ржЯрж┐',
                    'ржорзНржпрж╛рж▓ржУржпрж╝рзНржпрж╛рж░ ржЕрзНржпрж╛ржирж╛рж▓рж╛ржЗрж╕рж┐рж╕',
                    'ржЗржирж╕рж┐ржбрзЗржирзНржЯ рж░рзЗрж╕ржкржирзНрж╕',
                    'рж╕рж┐ржХрж┐ржЙрж░рж┐ржЯрж┐ ржЖрж░рзНржХрж┐ржЯрзЗржХржЪрж╛рж░',
                    'ржХрзНрж░рж┐ржкрзНржЯрзЛржЧрзНрж░рж╛ржлрж┐',
                    'рж╕рж┐рж╕рзНржЯрзЗржо рж╣рж╛рж░рзНржбрзЗржирж┐ржВ',
                    'ржерзНрж░рзЗржЯ рж╣рж╛ржирзНржЯрж┐ржВ',
                    'рж╕рж┐ржХрж┐ржЙрж░рж┐ржЯрж┐ ржХржоржкрзНрж▓рж╛ржпрж╝рзЗржирзНрж╕'
                ]
            }),
            'рж╣рж╛ржирзНржЯрж╛рж░': AgentPersonality('рж╣рж╛ржирзНржЯрж╛рж░', {
                'base_prompt': 'ржЖржорж┐ рж╣рж╛ржирзНржЯрж╛рж░, рж╕ржорж╕рзНржпрж╛ ржЦрзБржБржЬрзЗ ржмрзЗрж░ ржХрж░рж┐ ржПржмржВ рж╕ржорж╛ржзрж╛ржи ржХрж░рж┐ред',
                'style': 'рж╣рж╛ржирзНржЯрж╛рж░, рж╕рждрж░рзНржХ ржПржмржВ ржжржХрзНрж╖ред рж╕ржорж╕рзНржпрж╛ ржЦрзБржБржЬрзЗ ржмрзЗрж░ ржХрж░рж┐ред',
                'expertise': 'ржмрж╛ржЧ рж╣рж╛ржирзНржЯрж┐ржВ, ржХрзЛржб рж░рж┐ржнрж┐ржЙ, ржХрзЛржпрж╝рж╛рж▓рж┐ржЯрж┐ ржЕрзНржпрж╛рж╕рзБрж░рзЗржирзНрж╕',
                'emoji': 'ЁЯФН',
                'color': 'red',
                'capabilities': [
                    'ржмрж╛ржЧ рж╣рж╛ржирзНржЯрж┐ржВ ржПржмржВ ржбрж┐ржмрж╛ржЧрж┐ржВ',
                    'ржХрзЛржб ржХрзЛржпрж╝рж╛рж▓рж┐ржЯрж┐ ржЕрзНржпрж╛рж╕рзБрж░рзЗржирзНрж╕',
                    'ржЕржЯрзЛржорзЗржЯрзЗржб ржЯрзЗрж╕рзНржЯрж┐ржВ',
                    'ржХрзЛржб рж░рж┐ржнрж┐ржЙ ржПржмржВ ржЕрзНржпрж╛ржирж╛рж▓рж╛ржЗрж╕рж┐рж╕',
                    'ржкрж╛рж░ржлрж░ржорзНржпрж╛ржирзНрж╕ ржкрзНрж░рзЛржлрж╛ржЗрж▓рж┐ржВ',
                    'ржорзЗржорж░рж┐ рж▓рж┐ржХ ржбрж┐ржЯрзЗржХрж╢ржи',
                    'ржХрзЛржб ржХржоржкрзНрж▓рзЗржХрзНрж╕рж┐ржЯрж┐ ржЕрзНржпрж╛ржирж╛рж▓рж╛ржЗрж╕рж┐рж╕',
                    'ржЯрзЗрж╕рзНржЯ ржХржнрж╛рж░рзЗржЬ ржЕрзНржпрж╛ржирж╛рж▓рж╛ржЗрж╕рж┐рж╕',
                    'ржХрзЛржб рж╕рзНржЯрзНржпрж╛ржЯрж┐ржХ ржЕрзНржпрж╛ржирж╛рж▓рж╛ржЗрж╕рж┐рж╕',
                    'ржХрзЛржпрж╝рж╛рж▓рж┐ржЯрж┐ ржорзЗржЯрзНрж░рж┐ржХрзНрж╕'
                ]
            })
        }
        
        # System Status
        self.system_status = {
            'ollama_connected': False,
            'available_models': [],
            'last_health_check': None,
            'active_agents': set(),
            'performance_stats': {},
            'auto_response_enabled': True
        }
        
        # Initialize system
        self.initialize_system()
        
        # Start memory cleanup thread
        self.start_memory_cleanup()
        
    def initialize_system(self):
        """Initialize the system with health checks"""
        logger.info("ЁЯЪА Initializing Advanced Agent System...")
        
        # Check Ollama connection
        self.check_ollama_connection()
        
        # Log initialization
        logger.info(f"ЁЯОн System: {self.name}")
        logger.info(f"ЁЯСитАНЁЯСйтАНЁЯСз Agents: {list(self.agent_personalities.keys())}")
        logger.info(f"ЁЯМР Ollama URL: {self.ollama_url}")
        logger.info(f"ЁЯдЦ Available Models: {self.system_status['available_models']}")
        
    def check_ollama_connection(self):
        """Check Ollama server connection with timeout"""
        try:
            response = requests.get(f"{self.ollama_url}/api/tags", timeout=5)
            if response.status_code == 200:
                models_data = response.json()
                self.system_status['available_models'] = [
                    model.get('name', '') for model in models_data.get('models', [])
                ]
                self.system_status['ollama_connected'] = True
                self.system_status['last_health_check'] = datetime.now()
                logger.info(f"тЬЕ Ollama connected. Available models: {self.system_status['available_models']}")
                return True
            else:
                logger.error(f"тЭМ Ollama API error: {response.status_code}")
                return False
        except Exception as e:
            logger.error(f"тЭМ Ollama connection failed: {e}")
            self.system_status['ollama_connected'] = False
            return False
            
    def start_memory_cleanup(self):
        """Start background memory cleanup thread"""
        def cleanup_worker():
            while True:
                try:
                    time.sleep(300)  # Cleanup every 5 minutes
                    self.memory_manager.cleanup_memory()
                    self.update_performance_stats()
                    logger.info("ЁЯз╣ Memory cleanup completed")
                except Exception as e:
                    logger.error(f"тЭМ Memory cleanup error: {e}")
                    
        cleanup_thread = threading.Thread(target=cleanup_worker, daemon=True)
        cleanup_thread.start()
        logger.info("ЁЯз╣ Memory cleanup thread started")
        
    def update_performance_stats(self):
        """Update performance statistics"""
        self.system_status['performance_stats'] = self.memory_manager.get_memory_stats()
        
    def call_cloud_fallback(self, prompt: str) -> Optional[str]:
        """Call cloud AI providers as fallback"""
        try:
            logger.info("ЁЯМР Trying cloud fallback providers...")
            
            # Import AI providers
            import sys
            sys.path.append('our-server')
            from ai_providers import AIProviders
            
            providers = AIProviders()
            available_providers = providers.get_available_providers()
            
            if not available_providers:
                logger.error("тЭМ No cloud providers available")
                return None
                
            # Try each provider
            for provider_name in available_providers:
                try:
                    logger.info(f"ЁЯМР Trying {provider_name}...")
                    
                    if provider_name == 'openrouter':
                        response = providers.call_openrouter(prompt)
                    elif provider_name == 'together':
                        response = providers.call_together(prompt)
                    elif provider_name == 'huggingface':
                        response = providers.call_huggingface(prompt)
                    elif provider_name == 'anthropic':
                        response = providers.call_anthropic(prompt)
                    else:
                        continue
                        
                    if response:
                        logger.info(f"тЬЕ Cloud fallback successful with {provider_name}")
                        return response
                        
                except Exception as e:
                    logger.warning(f"тЪая╕П {provider_name} failed: {e}")
                    continue
                    
            logger.error("тЭМ All cloud providers failed")
            return None
            
        except Exception as e:
            logger.error(f"тЭМ Cloud fallback error: {e}")
            return None
            
    def _is_complex_prompt(self, prompt: str) -> bool:
        """Check if prompt is complex and needs cloud fallback"""
        complex_keywords = [
            'analyze', 'review', 'optimize', 'debug', 'security', 'performance',
            'architecture', 'design', 'complex', 'advanced', 'sophisticated',
            'critical', 'important', 'urgent', 'production', 'enterprise'
        ]
        
        prompt_lower = prompt.lower()
        complexity_score = sum(1 for keyword in complex_keywords if keyword in prompt_lower)
        
        # Consider prompt complex if it has 2+ complex keywords or is long
        return complexity_score >= 2 or len(prompt) > 500
        
    def call_local_ai(self, prompt: str, model: str = None) -> Optional[str]:
        """Call local Ollama AI with performance optimization"""
        if not self.system_status['ollama_connected']:
            logger.error("тЭМ Ollama not connected")
            # Try cloud fallback
            return self.call_cloud_fallback(prompt)
            
        # Smart routing: Check if prompt is complex and needs cloud fallback
        if self._is_complex_prompt(prompt):
            logger.info("ЁЯМР Complex prompt detected, trying cloud fallback first")
            cloud_response = self.call_cloud_fallback(prompt)
            if cloud_response:
                return cloud_response

        # Use default model if none specified
        if model is None:
            model = self.default_model

        # Check if model is available
        if model not in self.system_status['available_models']:
            logger.warning(f"тЪая╕П Model {model} not available, using first available model")
            if self.system_status['available_models']:
                model = self.system_status['available_models'][0]
            else:
                logger.error("тЭМ No models available")
                return None

        try:
            logger.info(f"ЁЯдЦ Calling Ollama with model: {model}")

            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json={
                    "model": model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "num_predict": 500,
                        "temperature": 0.7,
                        "top_p": 0.9
                    }
                },
                timeout=30
            )

            if response.status_code == 200:
                result = response.json()
                response_text = result.get("response", "")
                logger.info(f"тЬЕ Local AI response received from {model}")
                return response_text
            else:
                logger.error(f"тЭМ Ollama API error: {response.status_code} - {response.text}")
                return None

        except requests.exceptions.Timeout:
            logger.error("тЭМ Ollama API timeout")
            return None
        except requests.exceptions.ConnectionError:
            logger.error("тЭМ Cannot connect to Ollama server")
            self.system_status['ollama_connected'] = False
            return None
        except Exception as e:
            logger.error(f"тЭМ Local AI error: {e}")
            # Try cloud fallback
            return self.call_cloud_fallback(prompt)

    def process_message(self, message: str, agent_name: str = 'рж╕рж╛рж╣ржи ржнрж╛ржЗ') -> Dict[str, Any]:
        """Process message with agent personality and memory management"""
        start_time = time.time()
        
        logger.info(f"ЁЯУЭ Processing message from {agent_name}: {message[:50]}...")
        
        # Get agent personality
        if agent_name not in self.agent_personalities:
            agent_name = 'рж╕рж╛рж╣ржи ржнрж╛ржЗ'  # Default fallback
            
        agent = self.agent_personalities[agent_name]
        
        # Get conversation context
        context = self.memory_manager.get_context()
        
        # Create personalized prompt
        prompt = agent.get_prompt(message, context)
        
        # Call local AI
        response = self.call_local_ai(prompt)
        
        processing_time = time.time() - start_time
        
        if response:
            logger.info(f"тЬЕ {agent_name} response successful ({processing_time:.2f}s)")
            
            # Add to memory
            self.memory_manager.add_to_history(message, response, agent_name)
            
            # Update active agents
            self.system_status['active_agents'].add(agent_name)
            
            return {
                "response": response,
                "agent": agent_name,
                "agent_emoji": agent.personality.get('emoji', 'ЁЯдЦ'),
                "agent_color": agent.personality.get('color', 'blue'),
                "agent_capabilities": agent.personality.get('capabilities', []),
                "processing_time": processing_time,
                "source": "local_ai",
                "model_used": self.default_model,
                "timestamp": datetime.now().isoformat(),
                "memory_stats": self.memory_manager.get_memory_stats()
            }
        else:
            logger.warning(f"тЪая╕П {agent_name} failed, returning fallback response")
            return {
                "response": f"ржнрж╛ржЗ, ржЖржорж┐ ржПржЦржиржЗ ржЖржкржирж╛рж░ ржкрзНрж░рж╢рзНржирзЗрж░ ржЙрждрзНрждрж░ ржжрж┐рждрзЗ ржкрж╛рж░ржЫрж┐ ржирж╛ред ржжржпрж╝рж╛ ржХрж░рзЗ ржПржХржЯрзБ ржкрж░рзЗ ржЖржмрж╛рж░ ржЪрзЗрж╖рзНржЯрж╛ ржХрж░рзБржиред",
                "agent": agent_name,
                "agent_emoji": agent.personality.get('emoji', 'ЁЯдЦ'),
                "agent_color": agent.personality.get('color', 'blue'),
                "agent_capabilities": agent.personality.get('capabilities', []),
                "processing_time": processing_time,
                "source": "fallback",
                "error": "local_ai_unavailable",
                "timestamp": datetime.now().isoformat(),
                "memory_stats": self.memory_manager.get_memory_stats()
            }

    def get_status(self) -> Dict[str, Any]:
        """Get comprehensive system status"""
        self.update_performance_stats()
        
        # Convert set to list for JSON serialization
        system_status_copy = self.system_status.copy()
        if 'active_agents' in system_status_copy:
            system_status_copy['active_agents'] = list(system_status_copy['active_agents'])
        
        return {
            "system": self.name,
            "agents": {
                name: {
                    "name": name,
                    "emoji": agent.personality.get('emoji', 'ЁЯдЦ'),
                    "color": agent.personality.get('color', 'blue'),
                    "capabilities": agent.personality.get('capabilities', []),
                    "is_loaded": agent.is_loaded,
                    "load_time": agent.load_time.isoformat() if agent.load_time else None
                }
                for name, agent in self.agent_personalities.items()
            },
            "system_status": system_status_copy,
            "memory_stats": self.memory_manager.get_memory_stats(),
            "ollama_url": self.ollama_url,
            "default_model": self.default_model,
            "timestamp": datetime.now().isoformat()
        }

    def get_info(self) -> Dict[str, Any]:
        """Get system information"""
        return {
            "name": self.name,
            "description": "Advanced Agent System with Lazy Loading and Memory Management",
            "version": "3.0.0",
            "features": [
                "Lazy Loading",
                "Memory Management", 
                "Agent Personalities with 10 Capabilities Each",
                "Performance Optimization",
                "Resource Management",
                "Auto-Response System"
            ],
            "agents": list(self.agent_personalities.keys()),
            "endpoints": {
                "chat": "/chat",
                "status": "/status",
                "info": "/info"
            }
        }

# Initialize Flask app
app = Flask(__name__)
CORS(app)

# Initialize advanced agent system
advanced_agent = AdvancedAgentSystem()

@app.route('/')
def home():
    """Home endpoint"""
    return jsonify({
        "system": advanced_agent.name,
        "endpoints": {
            "chat": "/chat",
            "info": "/info",
            "status": "/status"
        },
        "agents": {
            name: {
                "name": name,
                "emoji": agent.personality.get('emoji', 'ЁЯдЦ'),
                "color": agent.personality.get('color', 'blue'),
                "capabilities": agent.personality.get('capabilities', [])
            }
            for name, agent in advanced_agent.agent_personalities.items()
        },
        "ollama_status": advanced_agent.system_status['ollama_connected'],
        "available_models": advanced_agent.system_status['available_models']
    })

@app.route('/chat', methods=['POST'])
def chat():
    """Chat endpoint with agent selection"""
    try:
        data = request.get_json()
        message = data.get('message', '')
        agent = data.get('agent', 'рж╕рж╛рж╣ржи ржнрж╛ржЗ')

        if not message:
            return jsonify({"error": "Message is required"}), 400

        logger.info(f"ЁЯТм Chat request from {agent}: {message[:50]}...")

        result = advanced_agent.process_message(message, agent)

        logger.info(f"тЬЕ Chat response sent to {agent}")
        return jsonify(result)

    except Exception as e:
        logger.error(f"тЭМ Chat endpoint error: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/status')
def status():
    """Status endpoint"""
    return jsonify(advanced_agent.get_status())

@app.route('/info')
def info():
    """Info endpoint"""
    return jsonify(advanced_agent.get_info())

if __name__ == '__main__':
    logger.info("ЁЯдЦ Starting ZombieCoder Advanced Agent System...")
    logger.info(f"ЁЯОн System: {advanced_agent.name}")
    logger.info(f"ЁЯСитАНЁЯСйтАНЁЯСз Agents: {list(advanced_agent.agent_personalities.keys())}")
    logger.info("ЁЯМР Server starting on http://localhost:8004")
    logger.info("ЁЯУб Available endpoints:")
    logger.info("   - GET  / (home)")
    logger.info("   - POST /chat (chat with agents)")
    logger.info("   - GET  /status (system status)")
    logger.info("   - GET  /info (system info)")
    logger.info("=" * 50)

    app.run(host='0.0.0.0', port=8004, debug=True)
